# ğŸ‰ Your New Galen Translation Evaluation System

## âœ… What We Built

A **clean, modern, production-ready** system that takes your `10_chunks.txt` file and:

1. **Extracts** Greek text and reference translations automatically
2. **Translates** using OpenAI, Claude, and Gemini APIs
3. **Evaluates** against both reference translations using 6+ metrics
4. **Reports** results in clear, actionable formats

## ğŸ“ Complete File Structure

```
galen_eval/                          â† Your new clean project folder
â”‚
â”œâ”€â”€ ğŸ“– QUICKSTART.md                 â† START HERE! 5-minute setup guide
â”œâ”€â”€ ğŸ“– README.md                     â† Full documentation
â”œâ”€â”€ ğŸ“– PROJECT_SUMMARY.md            â† Design decisions & architecture
â”œâ”€â”€ ğŸ“– OVERVIEW.md                   â† This file
â”‚
â”œâ”€â”€ ğŸš€ pipeline.py                   â† MAIN SCRIPT - run this!
â”œâ”€â”€ ğŸ“‹ requirements.txt              â† Python dependencies
â”œâ”€â”€ ğŸ™ˆ .gitignore                    â† Git ignore rules
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ env.example                  â† Template for API keys
â”‚
â”œâ”€â”€ input/
â”‚   â””â”€â”€ 10_chunks.txt                â† Your input file (already here!)
â”‚
â”œâ”€â”€ output/                          â† All results go here
â”‚   â”œâ”€â”€ translations/                â† AI translation JSON files
â”‚   â”œâ”€â”€ evaluations/                 â† Evaluation scores JSON files
â”‚   â””â”€â”€ reports/                     â† Human-readable reports
â”‚
â””â”€â”€ src/                             â† Core modules
    â”œâ”€â”€ __init__.py                  â† Package init
    â”œâ”€â”€ parser.py                    â† Parse input documents
    â”œâ”€â”€ translator.py                â† Call translation APIs
    â”œâ”€â”€ evaluator.py                 â† Compute evaluation metrics
    â””â”€â”€ reporter.py                  â† Generate reports
```

## ğŸ¯ How to Use It

### Quick Start (5 minutes)

```bash
cd galen_eval

# 1. Run setup (creates venv, installs dependencies)
./setup.sh

# 2. Activate virtual environment
source venv/bin/activate

# 3. Make sure your .env has API keys
# The pipeline checks ../.env first, then config/.env
nano ../.env  # Edit project root .env

# 4. Run the pipeline
python3 pipeline.py input/10_chunks.txt

# Done! Check output/ for results

# 5. When finished
deactivate
```

### What You Get

After running the pipeline, you'll have:

```
output/
â”œâ”€â”€ translations/
â”‚   â””â”€â”€ 10_chunks_translations_TIMESTAMP.json    â† All 3 AI translations
â”‚
â”œâ”€â”€ evaluations/
â”‚   â””â”€â”€ 10_chunks_evaluation_TIMESTAMP.json      â† All evaluation scores
â”‚
â””â”€â”€ reports/
    â”œâ”€â”€ 10_chunks_summary_TIMESTAMP.txt          â† Quick overview
    â”œâ”€â”€ 10_chunks_detailed_TIMESTAMP.txt         â† Full report with examples
    â””â”€â”€ 10_chunks_scores_TIMESTAMP.csv           â† Spreadsheet-ready data
```

## ğŸ“Š Evaluation Metrics Included

All metrics are **FREE** (no API costs):

### Lexical Metrics (word/character matching)
- **BLEU-4** - N-gram precision (standard MT metric)
- **chrF++** - Character n-gram F-score with word bigrams
- **METEOR** - Incorporates stemming and synonyms
- **ROUGE-L** - Longest common subsequence

### Neural/Semantic Metrics (meaning-based)
- **BERTScore** - Neural contextual embeddings
- **COMET** - Neural reference-based metric (requires source)
- **BLEURT** - Learned evaluation metric (Linux only, optional)

## ğŸ”¥ Key Features

### 1. **Automatic Parsing**
- Automatically detects Greek text (by character set)
- Automatically identifies reference translations
- Validates everything before processing

### 2. **Multiple Models & Metrics**
- **3 AI models**: OpenAI GPT-4o, Claude Sonnet, Gemini 2.0
- **6 metrics**: BLEU-4, chrF++, METEOR, ROUGE-L, BERTScore, COMET
- Evaluates against **multiple references** for robust scoring

### 3. **Flexible & Configurable**
```bash
# Use only specific models
python pipeline.py input/10_chunks.txt --models openai claude

# Use only fast metrics
python pipeline.py input/10_chunks.txt --metrics bleu rouge chrf

# Enable GPU for faster neural metrics
python pipeline.py input/10_chunks.txt --gpu

# Parallel translation (faster)
python pipeline.py input/10_chunks.txt --parallel
```

### 4. **Clear Output**
- Summary report with overall rankings
- Detailed report with translation examples
- CSV export for statistical analysis
- JSON for programmatic access

## ğŸ†š Old vs New System

| Feature | Old System | âœ¨ New System |
|---------|-----------|-------------|
| **Entry point** | Unclear, multiple scripts | `pipeline.py` (one command) |
| **Structure** | Scattered in archive/ | Clean `galen_eval/` folder |
| **Documentation** | Multiple READMEs | QUICKSTART.md + detailed docs |
| **Parsing** | Manual/error-prone | Automatic with validation |
| **Output** | JSON only | JSON + TXT + CSV + reports |
| **Error handling** | Crashes | Graceful degradation |
| **Testing** | Hard to test parts | Each module independent |
| **Learning curve** | Steep | 5-minute quick start |

## ğŸ’° Cost Information

**Metrics: $0** (all free and open-source!)

**Translation APIs** (approximate costs per 10 chunks):
- OpenAI GPT-4o: ~$0.10
- Claude Sonnet: ~$0.15  
- Gemini 2.0: ~$0.05

**Total for 10 chunks: ~$0.30**

For your 10-chunk test file, expect to spend about **30 cents** for all three translations.

## ğŸ“ Example Output

Here's what a summary report looks like:

```
================================================================================
TRANSLATION EVALUATION SUMMARY
================================================================================

ğŸ† OVERALL MODEL RANKINGS
--------------------------------------------------------------------------------
ğŸ¥‡ 1. GEMINI            0.5435
ğŸ¥ˆ 2. CLAUDE            0.5271
ğŸ¥‰ 3. OPENAI            0.5054

ğŸ“Š BEST MODEL PER METRIC
--------------------------------------------------------------------------------
  BLEU-4         â†’ GEMINI     (0.2648)
  chrF++         â†’ GEMINI     (0.6621)
  METEOR         â†’ CLAUDE     (0.5234)
  ROUGE-L        â†’ CLAUDE     (0.6844)
  BERTScore      â†’ CLAUDE     (0.8654)
  COMET          â†’ GEMINI     (0.8910)

ğŸ“ˆ DETAILED SCORES BY MODEL
--------------------------------------------------------------------------------

CLAUDE:
  BLEU-4         0.2521 Â± 0.0823 (min=0.145, max=0.412, n=10)
  chrF++         0.6523 Â± 0.0612 (min=0.523, max=0.745, n=10)
  METEOR         0.5234 Â± 0.0534 (min=0.412, max=0.621, n=10)
  ROUGE-L        0.6844 Â± 0.0521 (min=0.612, max=0.801, n=10)
  ...
```

## ğŸš¦ Next Steps

### To Run Your First Evaluation:

1. **Read QUICKSTART.md** (5 minutes)
2. **Set up API keys** in `config/.env`
3. **Run the pipeline**: `python pipeline.py input/10_chunks.txt`
4. **Check results** in `output/reports/`

### To Understand the System:

1. **PROJECT_SUMMARY.md** - Architecture and design decisions
2. **README.md** - Complete reference documentation
3. Individual module docs - Each Python file has detailed docstrings

### To Customize:

1. **Edit pipeline.py** - Change default models/metrics
2. **Extend evaluator.py** - Add new metrics
3. **Extend translator.py** - Add new translation models
4. **Modify reporter.py** - Customize report formats

## âœ¨ What Makes This Special

1. **One Command to Rule Them All**: `python pipeline.py input/10_chunks.txt`
2. **Works Out of the Box**: Your input file is already in the right format
3. **Multiple References**: Uses both reference translations for robust evaluation
4. **6 Core Metrics**: BLEU-4, chrF++, METEOR, ROUGE-L, BERTScore, COMET
5. **Clear Results**: Not just numbers - actual translation examples and analysis
6. **Production Ready**: Error handling, logging, validation, documentation
7. **Extensible**: Clean modular design makes it easy to add features

## ğŸ Bonus Features

- **CSV export** for statistical analysis in R/Python/Excel
- **Modular design** - use components independently
- **Validation** - automatic input format checking
- **Graceful degradation** - if one API fails, others continue
- **GPU support** - optional acceleration for neural metrics
- **Parallel translation** - faster API calls (optional)

## ğŸ“š Documentation Quality

Every file is thoroughly documented:
- âœ… Module-level docstrings explain purpose
- âœ… Function-level docstrings explain parameters and returns
- âœ… Inline comments explain complex logic
- âœ… Type hints for better IDE support
- âœ… Multiple README files for different use cases

## ğŸ Ready to Go!

Everything is set up and ready. Your `10_chunks.txt` file is already in the `input/` folder.

**Just run:**
```bash
cd galen_eval
./setup.sh                      # Creates venv, installs dependencies
source venv/bin/activate        # Activate virtual environment
# Make sure ../.env has your API keys
python3 pipeline.py input/10_chunks.txt
```

**That's it!** ğŸš€

**Important:** Always activate the virtual environment before running the pipeline:
```bash
source venv/bin/activate
```

---

## Questions?

- **Setup issues?** â†’ Check QUICKSTART.md troubleshooting section
- **How does it work?** â†’ Read PROJECT_SUMMARY.md
- **What can I customize?** â†’ Check README.md
- **Individual components?** â†’ Each .py file has `--help`

## Summary

You now have a **professional-grade, research-ready** translation evaluation system that:

âœ… Takes your existing input format  
âœ… Uses cutting-edge AI models  
âœ… Applies industry-standard metrics  
âœ… Produces publication-quality results  
âœ… Is fully documented and extensible  

**All in one clean, self-contained folder: `galen_eval/`**

Enjoy! ğŸ‰ğŸ“ŠğŸ›ï¸
