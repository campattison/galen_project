# Table 1A: Key Automated MT Evaluation Scores (Condensed)

*Generated: 2026-01-22*

**For main text.** Full metrics (including METEOR, ROUGE-L) in Appendix.

| Text | Model | BLEU-4 | chrF++ | BERTScore | COMET |
|------|-------|-------|-------|-------|-------|
| *Mixtures* | ChatGPT | 31.4 (± 5.8) | 53.4 (± 3.5) | 91.0 (± 1.1) | 79.9 (± 1.8) |
|  | Claude | 34.2 (± 5.9) | 55.4 (± 3.2) | 91.6 (± 0.9) | 79.8 (± 2.0) |
|  | Gemini | 34.2 (± 4.7) | 57.0 (± 2.7) | 91.5 (± 1.0) | 80.7 (± 1.7) |
| *Comp.* | ChatGPT | 15.7 (± 5.1) | 47.4 (± 5.0) | 89.1 (± 2.0) | 75.1 (± 3.8) |
|  | Claude | 16.7 (± 4.5) | 49.4 (± 2.3) | 89.7 (± 1.5) | 76.5 (± 2.2) |
|  | Gemini | 19.0 (± 3.8) | 51.2 (± 2.9) | 89.9 (± 1.3) | 77.3 (± 2.2) |