# Table 2: Aggregate Human Evaluation Scores (MQM & TQS)

*Generated: 2026-01-22*

**Purpose:** Present the 'ground truth' of MT quality established by humans in the loop.

**Status:** TEMPLATE - To be populated after human evaluation is complete.

| Model | Mix. TQS | Mix. High % | Mix. Low % | Mix. Fail % | Mix. Crit. | Comp. TQS | Comp. High % | Comp. Low % | Comp. Fail % | Comp. Crit. |
|-------|----------|-------------|------------|-------------|------------|-----------|--------------|-------------|--------------|-------------|
| ChatGPT | — | — | — | — | — | — | — | — | — | — |
| Claude | — | — | — | — | — | — | — | — | — | — |
| Gemini | — | — | — | — | — | — | — | — | — | — |
| *Aggregate* | — | — | — | — | — | — | — | — | — | — |

**Column definitions:**
- **TQS**: Translation Quality Score, mean (± SD)
- **High %**: Percentage of chunks with high-pass quality
- **Low %**: Percentage of chunks with low-pass quality
- **Fail %**: Percentage of chunks that failed quality threshold
- **Crit.**: Count of critical errors identified

**MQM Error Categories to track:**
- Accuracy: Addition, Omission, Mistranslation, Untranslated
- Fluency: Grammar, Spelling, Punctuation, Register
- Terminology: Inconsistent, Wrong term
- Style: Awkward, Unnatural