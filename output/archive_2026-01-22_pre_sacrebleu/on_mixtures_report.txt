================================================================================
TRANSLATION EVALUATION SUMMARY
================================================================================

üèÜ OVERALL MODEL RANKINGS
--------------------------------------------------------------------------------
ü•á 1. GEMINI          0.6165
ü•à 2. CLAUDE          0.6084
ü•â 3. OPENAI          0.5893

üìä BEST MODEL PER METRIC
--------------------------------------------------------------------------------
  BERTScore       ‚Üí CLAUDE     (0.9159)
  BLEU-4          ‚Üí GEMINI     (0.3470)
  COMET           ‚Üí GEMINI     (0.8066)
  METEOR          ‚Üí GEMINI     (0.4999)
  ROUGE-L         ‚Üí GEMINI     (0.5597)
  chrF++          ‚Üí GEMINI     (0.5702)

üìà DETAILED SCORES BY MODEL
--------------------------------------------------------------------------------

CLAUDE:
  BERTScore       0.9159 ¬± 0.0089 (min=0.902, max=0.933, n=10)
  BLEU-4          0.3456 ¬± 0.0592 (min=0.248, max=0.463, n=10)
  COMET           0.7976 ¬± 0.0195 (min=0.772, max=0.830, n=10)
  METEOR          0.4846 ¬± 0.0320 (min=0.431, max=0.558, n=10)
  ROUGE-L         0.5532 ¬± 0.0533 (min=0.499, max=0.674, n=10)
  chrF++          0.5536 ¬± 0.0323 (min=0.508, max=0.624, n=10)

GEMINI:
  BERTScore       0.9154 ¬± 0.0096 (min=0.903, max=0.934, n=10)
  BLEU-4          0.3470 ¬± 0.0493 (min=0.290, max=0.445, n=10)
  COMET           0.8066 ¬± 0.0168 (min=0.776, max=0.832, n=10)
  METEOR          0.4999 ¬± 0.0373 (min=0.447, max=0.550, n=10)
  ROUGE-L         0.5597 ¬± 0.0465 (min=0.505, max=0.670, n=10)
  chrF++          0.5702 ¬± 0.0268 (min=0.542, max=0.632, n=10)

OPENAI:
  BERTScore       0.9101 ¬± 0.0111 (min=0.896, max=0.931, n=10)
  BLEU-4          0.3190 ¬± 0.0589 (min=0.246, max=0.425, n=10)
  COMET           0.7990 ¬± 0.0185 (min=0.765, max=0.828, n=10)
  METEOR          0.4644 ¬± 0.0508 (min=0.365, max=0.547, n=10)
  ROUGE-L         0.5092 ¬± 0.0500 (min=0.456, max=0.621, n=10)
  chrF++          0.5343 ¬± 0.0347 (min=0.497, max=0.618, n=10)

üìã METHODOLOGY
--------------------------------------------------------------------------------
  Multi-reference evaluation:
  ‚Ä¢ BLEU-4, chrF++, METEOR: n-grams matched against ANY reference
  ‚Ä¢ ROUGE-L, BERTScore, COMET: MAX across references

‚ÑπÔ∏è  EVALUATION INFO
--------------------------------------------------------------------------------
  Chunks evaluated: 10
  Models: claude, gemini, openai
  Reference translations: 2
  Total evaluations: 30
